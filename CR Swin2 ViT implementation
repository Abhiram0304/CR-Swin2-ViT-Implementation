{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6897390,"sourceType":"datasetVersion","datasetId":3962066},{"sourceId":7165942,"sourceType":"datasetVersion","datasetId":4139552},{"sourceId":7549906,"sourceType":"datasetVersion","datasetId":4394153},{"sourceId":7551226,"sourceType":"datasetVersion","datasetId":4343232},{"sourceId":7636108,"sourceType":"datasetVersion","datasetId":4449867},{"sourceId":8031339,"sourceType":"datasetVersion","datasetId":4733880}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys \nsys.path.append('/kaggle/input/swinvitmodel/vizviva_fets_2022-main/vizviva_fets_2022-main')\nsys.path.append('/kaggle/input/swinvitmodel/vizviva_fets_2022-main/vizviva_fets_2022-main/crswin2')","metadata":{"execution":{"iopub.status.busy":"2024-04-01T19:11:16.056345Z","iopub.execute_input":"2024-04-01T19:11:16.056689Z","iopub.status.idle":"2024-04-01T19:11:16.068326Z","shell.execute_reply.started":"2024-04-01T19:11:16.056661Z","shell.execute_reply":"2024-04-01T19:11:16.067331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install fvcore\n!pip install monai\n!pip install einops\n!pip install nnunet\n!pip install axial-attention\n!pip install pyyaml","metadata":{"execution":{"iopub.status.busy":"2024-04-01T19:11:16.069765Z","iopub.execute_input":"2024-04-01T19:11:16.070156Z","iopub.status.idle":"2024-04-01T19:12:57.282952Z","shell.execute_reply.started":"2024-04-01T19:11:16.070127Z","shell.execute_reply":"2024-04-01T19:12:57.281905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --use-pep517 mmcv-full==1.3.14","metadata":{"execution":{"iopub.status.busy":"2024-04-01T19:12:57.284622Z","iopub.execute_input":"2024-04-01T19:12:57.284987Z","iopub.status.idle":"2024-04-01T19:13:36.624984Z","shell.execute_reply.started":"2024-04-01T19:12:57.284948Z","shell.execute_reply":"2024-04-01T19:13:36.624004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2024-04-01T19:13:36.627355Z","iopub.execute_input":"2024-04-01T19:13:36.627679Z","iopub.status.idle":"2024-04-01T19:13:39.937930Z","shell.execute_reply.started":"2024-04-01T19:13:36.627649Z","shell.execute_reply":"2024-04-01T19:13:39.936916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# coding=utf-8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport logging\n\nimport torch\nimport torch.nn as nn\n\nfrom cr_swin_2 import SwinTransformerSys3D\n\nlogger = logging.getLogger(__name__)\n\n\nclass VTUNet(nn.Module):\n    def __init__(self, config, num_classes=3, zero_head=False, embed_dim=48, win_size=7):\n        super(VTUNet, self).__init__()\n        self.num_classes = num_classes\n        self.zero_head = zero_head\n        self.config = config\n        self.embed_dim = embed_dim\n        self.win_size = win_size\n        self.win_size = (self.win_size,self.win_size,self.win_size)\n\n        self.swin_unet = SwinTransformerSys3D(img_size=(128, 128, 128),\n                                            patch_size=(4, 4, 4),\n                                            in_chans=4,\n                                            num_classes=self.num_classes,\n                                            embed_dim=self.embed_dim,\n                                            depths=[2, 2, 2, 1],\n                                            depths_decoder=[1, 2, 2, 2],\n                                            num_heads=[3, 6, 12, 24],\n                                            window_size=self.win_size,\n                                            mlp_ratio=4.,\n                                            qkv_bias=True,\n                                            qk_scale=None,\n                                            drop_rate=0.,\n                                            attn_drop_rate=0.,\n                                            drop_path_rate=0.2,\n                                            norm_layer=nn.LayerNorm,\n                                            patch_norm=True,\n                                            use_checkpoint=False,\n                                            frozen_stages=-1,\n                                            final_upsample=\"expand_first\")\n\n    def forward(self, x):\n        logits = self.swin_unet(x)\n        return logits\n\n    def load_from(self, config):\n        pretrained_path = config.MODEL.PRETRAIN_CKPT\n        if pretrained_path is not None:\n            print(\"pretrained_path:{}\".format(pretrained_path))\n            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n            pretrained_dict = torch.load(pretrained_path, map_location=device)\n            if \"model\" not in pretrained_dict:\n                print(\"---start load pretrained modle by splitting---\")\n                pretrained_dict = {k[17:]: v for k, v in pretrained_dict.items()}\n                for k in list(pretrained_dict.keys()):\n                    if \"output\" in k:\n                        print(\"delete key:{}\".format(k))\n                        del pretrained_dict[k]\n                self.swin_unet.load_state_dict(pretrained_dict, strict=False)\n\n                return\n            pretrained_dict = pretrained_dict['model']\n            print(\"---start load pretrained modle of swin encoder---\")\n\n            model_dict = self.swin_unet.state_dict()\n            full_dict = copy.deepcopy(pretrained_dict)\n            for k, v in pretrained_dict.items():\n                if \"layers.\" in k:\n                    current_layer_num = 3 - int(k[7:8])\n                    current_k = \"layers_up.\" + str(current_layer_num) + k[8:]\n                    full_dict.update({current_k: v})\n            for k in list(full_dict.keys()):\n                if k in model_dict:\n                    if full_dict[k].shape != model_dict[k].shape:\n                        print(\"delete:{};shape pretrain:{};shape model:{}\".format(k, v.shape, model_dict[k].shape))\n                        del full_dict[k]\n\n            self.swin_unet.load_state_dict(full_dict, strict=False)\n        else:\n            print(\"none pretrain\")\n            \nprint(\"Done\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-01T19:13:39.939753Z","iopub.execute_input":"2024-04-01T19:13:39.940265Z","iopub.status.idle":"2024-04-01T19:13:46.583346Z","shell.execute_reply.started":"2024-04-01T19:13:39.940229Z","shell.execute_reply":"2024-04-01T19:13:46.582345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import yaml\nfrom types import SimpleNamespace\n\n# Load configuration from file\nwith open('/kaggle/input/swinvitmodel/vizviva_fets_2022-main/vizviva_fets_2022-main/configs/cr_swin2_vt.yaml', 'r') as file:\n    cfgs3_dict = yaml.safe_load(file)\n\n# Convert the dictionary to a SimpleNamespace object\ndef convert_to_namespace(d):\n    if isinstance(d, dict):\n        return SimpleNamespace(**{k: convert_to_namespace(v) for k, v in d.items()})\n    else:\n        return d\n\ncfgs3 = convert_to_namespace(cfgs3_dict)\nprint(cfgs3)\n\n# # Accessing attributes as object\n# print(cfgs3.MODEL.VT_UNET.DEPTHS)\n# print(cfgs3.MODEL.VT_UNET)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:49:00.153934Z","iopub.execute_input":"2024-03-15T09:49:00.154415Z","iopub.status.idle":"2024-03-15T09:49:00.177721Z","shell.execute_reply.started":"2024-03-15T09:49:00.154373Z","shell.execute_reply":"2024-03-15T09:49:00.176523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel = VTUNet(config=cfgs3)\ninputs = torch.rand(1, 4, 128, 128, 128)\noutputs = model(inputs)\nprint(len(outputs))\nprint(outputs.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:49:00.180588Z","iopub.execute_input":"2024-03-15T09:49:00.181480Z","iopub.status.idle":"2024-03-15T09:49:09.752905Z","shell.execute_reply.started":"2024-03-15T09:49:00.181434Z","shell.execute_reply":"2024-03-15T09:49:09.751771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom __future__ import annotations\n\nimport os\nimport shutil\nimport sys\nimport warnings\nfrom collections.abc import Callable, Sequence\nfrom pathlib import Path\nfrom typing import Any\n\nimport numpy as np\n\nfrom monai.apps.tcia import (\n    download_tcia_series_instance,\n    get_tcia_metadata,\n    get_tcia_ref_uid,\n    match_tcia_ref_uid_in_study,\n)\n# from monai.apps.utils import download_and_extract\nfrom monai.config.type_definitions import PathLike\nfrom monai.data import (\n    CacheDataset,\n    PydicomReader,\n    load_decathlon_datalist,\n    load_decathlon_properties,\n    partition_dataset,\n    select_cross_validation_folds,\n)\nfrom monai.transforms import LoadImaged, Randomizable\nfrom monai.utils import ensure_tuple\n\ndef _basename(p: PathLike) -> str:\n    \"\"\"get the last part of the path (removing the trailing slash if it exists)\"\"\"\n    sep = os.path.sep + (os.path.altsep or \"\") + \"/ \"\n    return Path(f\"{p}\".rstrip(sep)).name\n\ndef extractall(\n    filepath: PathLike,\n    output_dir: PathLike = \".\",\n    hash_val: str | None = None,\n    hash_type: str = \"md5\",\n    file_type: str = \"\",\n    has_base: bool = True,\n) -> None:\n    \"\"\"\n    Extract file to the output directory.\n    Expected file types are: `zip`, `tar.gz` and `tar`.\n\n    Args:\n        filepath: the file path of compressed file.\n        output_dir: target directory to save extracted files.\n        hash_val: expected hash value to validate the compressed file.\n            if None, skip hash validation.\n        hash_type: 'md5' or 'sha1', defaults to 'md5'.\n        file_type: string of file type for decompressing. Leave it empty to infer the type from the filepath basename.\n        has_base: whether the extracted files have a base folder. This flag is used when checking if the existing\n            folder is a result of `extractall`, if it is, the extraction is skipped. For example, if A.zip is unzipped\n            to folder structure `A/*.png`, this flag should be True; if B.zip is unzipped to `*.png`, this flag should\n            be False.\n\n    Raises:\n        RuntimeError: When the hash validation of the ``filepath`` compressed file fails.\n        NotImplementedError: When the ``filepath`` file extension is not one of [zip\", \"tar.gz\", \"tar\"].\n\n    \"\"\"\n    if has_base:\n        # the extracted files will be in this folder\n        cache_dir = Path(output_dir, _basename(filepath).split(\".\")[0])\n    else:\n        cache_dir = Path(output_dir)\n    if cache_dir.exists() and next(cache_dir.iterdir(), None) is not None:\n        logger.info(f\"Non-empty folder exists in {cache_dir}, skipped extracting.\")\n        return\n    filepath = Path(filepath)\n    if hash_val and not check_hash(filepath, hash_val, hash_type):\n        raise RuntimeError(\n            f\"{hash_type} check of compressed file failed: \" f\"filepath={filepath}, expected {hash_type}={hash_val}.\"\n        )\n    print(f\"Writing into directory: {output_dir}.\")\n    _file_type = file_type.lower().strip()\n    print(filepath, _file_type)\n    if filepath.name.endswith(\"zip\") or _file_type == \"zip\":\n        zip_file = zipfile.ZipFile(filepath)\n        zip_file.extractall(output_dir)\n        zip_file.close()\n        return\n    if filepath.name.endswith(\"tar\") or filepath.name.endswith(\"tar.gz\") or \"tar\" in _file_type:\n        tar_file = tarfile.open(filepath)\n        tar_file.extractall(output_dir)\n        tar_file.close()\n        return\n    raise NotImplementedError(\n        f'Unsupported file type, available options are: [\"zip\", \"tar.gz\", \"tar\"]. name={filepath} type={file_type}.'\n    )\n\n\ndef download_and_extract(\n    url: str,\n    filepath: PathLike = \"\",\n    output_dir: PathLike = \".\",\n    hash_val: str | None = None,\n    hash_type: str = \"md5\",\n    file_type: str = \"\",\n    has_base: bool = True,\n    progress: bool = True,\n) -> None:\n    \"\"\"\n    Download file from URL and extract it to the output directory.\n\n    Args:\n        url: source URL link to download file.\n        filepath: the file path of the downloaded compressed file.\n            use this option to keep the directly downloaded compressed file, to avoid further repeated downloads.\n        output_dir: target directory to save extracted files.\n            default is the current directory.\n        hash_val: expected hash value to validate the downloaded file.\n            if None, skip hash validation.\n        hash_type: 'md5' or 'sha1', defaults to 'md5'.\n        file_type: string of file type for decompressing. Leave it empty to infer the type from url's base file name.\n        has_base: whether the extracted files have a base folder. This flag is used when checking if the existing\n            folder is a result of `extractall`, if it is, the extraction is skipped. For example, if A.zip is unzipped\n            to folder structure `A/*.png`, this flag should be True; if B.zip is unzipped to `*.png`, this flag should\n            be False.\n        progress: whether to display progress bar.\n    \"\"\"\n    print()\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        filename = filepath or Path(tmp_dir, _basename(url)).resolve()\n#         shutil.copy(\"/kaggle/input/segresnet-data/BraTS-MEN-Train.zip\", tmp_dir)\n#         download_url(url=url, filepath=filename, hash_val=hash_val, hash_type=hash_type, progress=progress)\n        extractall(filepath=filename, output_dir=output_dir, file_type=file_type, has_base=has_base)\n        \nclass DecathlonDataset(Randomizable, CacheDataset):\n    \"\"\"\n    The Dataset to automatically download the data of Medical Segmentation Decathlon challenge\n    (http://medicaldecathlon.com/) and generate items for training, validation or test.\n    It will also load these properties from the JSON config file of dataset. user can call `get_properties()`\n    to get specified properties or all the properties loaded.\n    It's based on :py:class:`monai.data.CacheDataset` to accelerate the training process.\n\n    Args:\n        root_dir: user's local directory for caching and loading the MSD datasets.\n        task: which task to download and execute: one of list (\"Task01_BrainTumour\", \"Task02_Heart\",\n            \"Task03_Liver\", \"Task04_Hippocampus\", \"Task05_Prostate\", \"Task06_Lung\", \"Task07_Pancreas\",\n            \"Task08_HepaticVessel\", \"Task09_Spleen\", \"Task10_Colon\").\n        section: expected data section, can be: `training`, `validation` or `test`.\n        transform: transforms to execute operations on input data.\n            for further usage, use `EnsureChannelFirstd` to convert the shape to [C, H, W, D].\n        download: whether to download and extract the Decathlon from resource link, default is False.\n            if expected file already exists, skip downloading even set it to True.\n            user can manually copy tar file or dataset folder to the root directory.\n        val_frac: percentage of validation fraction in the whole dataset, default is 0.2.\n        seed: random seed to randomly shuffle the datalist before splitting into training and validation, default is 0.\n            note to set same seed for `training` and `validation` sections.\n        cache_num: number of items to be cached. Default is `sys.maxsize`.\n            will take the minimum of (cache_num, data_length x cache_rate, data_length).\n        cache_rate: percentage of cached data in total, default is 1.0 (cache all).\n            will take the minimum of (cache_num, data_length x cache_rate, data_length).\n        num_workers: the number of worker threads if computing cache in the initialization.\n            If num_workers is None then the number returned by os.cpu_count() is used.\n            If a value less than 1 is specified, 1 will be used instead.\n        progress: whether to display a progress bar when downloading dataset and computing the transform cache content.\n        copy_cache: whether to `deepcopy` the cache content before applying the random transforms,\n            default to `True`. if the random transforms don't modify the cached content\n            (for example, randomly crop from the cached image and deepcopy the crop region)\n            or if every cache item is only used once in a `multi-processing` environment,\n            may set `copy=False` for better performance.\n        as_contiguous: whether to convert the cached NumPy array or PyTorch tensor to be contiguous.\n            it may help improve the performance of following logic.\n        runtime_cache: whether to compute cache at the runtime, default to `False` to prepare\n            the cache content at initialization. See: :py:class:`monai.data.CacheDataset`.\n\n    Raises:\n        ValueError: When ``root_dir`` is not a directory.\n        ValueError: When ``task`` is not one of [\"Task01_BrainTumour\", \"Task02_Heart\",\n            \"Task03_Liver\", \"Task04_Hippocampus\", \"Task05_Prostate\", \"Task06_Lung\", \"Task07_Pancreas\",\n            \"Task08_HepaticVessel\", \"Task09_Spleen\", \"Task10_Colon\"].\n        RuntimeError: When ``dataset_dir`` doesn't exist and downloading is not selected (``download=False``).\n\n    Example::\n\n        transform = Compose(\n            [\n                LoadImaged(keys=[\"image\", \"label\"]),\n                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n                ScaleIntensityd(keys=\"image\"),\n                ToTensord(keys=[\"image\", \"label\"]),\n            ]\n        )\n\n        val_data = DecathlonDataset(\n            root_dir=\"./\", task=\"Task09_Spleen\", transform=transform, section=\"validation\", seed=12345, download=True\n        )\n\n        print(val_data[0][\"image\"], val_data[0][\"label\"])\n\n    \"\"\"\n\n    resource = {\n        \"Task01_BrainTumour\": \"/kaggle/input/segresnet-data/BraTS-MEN-Train\",\n        \"Task02_Heart\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task02_Heart.tar\",\n        \"Task03_Liver\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task03_Liver.tar\",\n        \"Task04_Hippocampus\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task04_Hippocampus.tar\",\n        \"Task05_Prostate\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task05_Prostate.tar\",\n        \"Task06_Lung\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task06_Lung.tar\",\n        \"Task07_Pancreas\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task07_Pancreas.tar\",\n        \"Task08_HepaticVessel\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task08_HepaticVessel.tar\",\n        \"Task09_Spleen\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\",\n        \"Task10_Colon\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task10_Colon.tar\",\n    }\n    md5 = {\n        \"Task01_BrainTumour\": \"240a19d752f0d9e9101544901065d872\",\n        \"Task02_Heart\": \"06ee59366e1e5124267b774dbd654057\",\n        \"Task03_Liver\": \"a90ec6c4aa7f6a3d087205e23d4e6397\",\n        \"Task04_Hippocampus\": \"9d24dba78a72977dbd1d2e110310f31b\",\n        \"Task05_Prostate\": \"35138f08b1efaef89d7424d2bcc928db\",\n        \"Task06_Lung\": \"8afd997733c7fc0432f71255ba4e52dc\",\n        \"Task07_Pancreas\": \"4f7080cfca169fa8066d17ce6eb061e4\",\n        \"Task08_HepaticVessel\": \"641d79e80ec66453921d997fbf12a29c\",\n        \"Task09_Spleen\": \"410d4a301da4e5b2f6f86ec3ddba524e\",\n        \"Task10_Colon\": \"bad7a188931dc2f6acf72b08eb6202d0\",\n    }\n\n    def __init__(\n        self,\n        root_dir: PathLike,\n        task: str,\n        section: str,\n        transform: Sequence[Callable] | Callable = (),\n        download: bool = False,\n        seed: int = 0,\n        val_frac: float = 0.2,\n        cache_num: int = sys.maxsize,\n        cache_rate: float = 1.0,\n        num_workers: int = 1,\n        progress: bool = True,\n        copy_cache: bool = True,\n        as_contiguous: bool = True,\n        runtime_cache: bool = False,\n    ) -> None:\n        root_dir = Path(root_dir)\n        if not root_dir.is_dir():\n            raise ValueError(\"Root directory root_dir must be a directory.\")\n        self.section = section\n        self.val_frac = val_frac\n        self.set_random_state(seed=seed)\n        if task not in self.resource:\n            raise ValueError(f\"Unsupported task: {task}, available options are: {list(self.resource.keys())}.\")\n        dataset_dir = root_dir / task\n#         tarfile_name = f\"{dataset_dir}.tar\"\n#         if download:\n#             download_and_extract(\n#                 url=self.resource[task],\n#                 filepath=tarfile_name,\n#                 output_dir=root_dir,\n#                 hash_val=self.md5[task],\n#                 hash_type=\"md5\",\n#                 progress=progress,\n#             )\n\n#         if not dataset_dir.exists():\n#             raise RuntimeError(\n#                 f\"Cannot find dataset directory: {dataset_dir}, please use download=True to download it.\"\n#             )\n#         dataset_dir = \"/kaggle/input/meningits-part1/brain-men-train1\"\n        self.indices: np.ndarray = np.array([])\n        data = self._generate_data_list(\"/kaggle/input/segres-json\")\n        # as `release` key has typo in Task04 config file, ignore it.\n        property_keys = [\n            \"name\",\n            \"description\",\n            \"reference\",\n            \"licence\",\n            \"tensorImageSize\",\n            \"modality\",\n            \"labels\",\n            \"numTraining\",\n            \"numTest\",\n        ]\n#         self._properties = load_decathlon_properties(\"/kaggle/input/segres-json/dataset.json\", property_keys)\n        if transform == ():\n            transform = LoadImaged([\"image\", \"label\"])\n        CacheDataset.__init__(\n            self,\n            data=data,\n            transform=transform,\n            cache_num=cache_num,\n            cache_rate=cache_rate,\n            num_workers=num_workers,\n            progress=progress,\n            copy_cache=copy_cache,\n            as_contiguous=as_contiguous,\n            runtime_cache=runtime_cache,\n        )\n\n\n# [docs]\n    def get_indices(self) -> np.ndarray:\n        \"\"\"\n        Get the indices of datalist used in this dataset.\n\n        \"\"\"\n        return self.indices\n\n\n\n\n# [docs]\n    def randomize(self, data: np.ndarray) -> None:\n        self.R.shuffle(data)\n\n\n\n\n# [docs]\n    def get_properties(self, keys: Sequence[str] | str | None = None) -> dict:\n        \"\"\"\n        Get the loaded properties of dataset with specified keys.\n        If no keys specified, return all the loaded properties.\n\n        \"\"\"\n        if keys is None:\n            return self._properties\n        if self._properties is not None:\n            return {key: self._properties[key] for key in ensure_tuple(keys)}\n        return {}\n\n\n\n    def _generate_data_list(self, dataset_dir: PathLike) -> list[dict]:\n        # the types of the item in data list should be compatible with the dataloader\n        dataset_dir = Path(dataset_dir) \n        section = \"training\" if self.section in [\"training\", \"validation\", \"test\"] else \"test\"\n        datalist = load_decathlon_datalist(\"/kaggle/input/segres-json/dataset (1).json\", True, section)\n#         datalist2 = load_decathlon_datalist(\"/kaggle/input/segres-json/dataset.json\", True, section)\n#         datalist = datalist.append(datalist2)\n#         print(datalist[898])\n#         print(\".............................................................................\")\n        return self._split_datalist(datalist)\n\n    def _split_datalist(self, datalist: list[dict]) -> list[dict]:\n#         if self.section == \"test\":\n#             return datalist\n        length = len(datalist)\n        indices = np.arange(length)\n        self.randomize(indices)\n\n        \n        val_length = int(length * self.val_frac)\n        \n        if self.section == \"training\":\n            self.indices = indices[val_length:]\n        elif self.section == \"validation\":\n            self.indices = indices[:100]\n        else:\n            self.indices = indices[100:246]\n#         print(self.indices)\n        return [datalist[i] for i in self.indices]","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:49:09.755073Z","iopub.execute_input":"2024-03-15T09:49:09.755509Z","iopub.status.idle":"2024-03-15T09:50:11.494524Z","shell.execute_reply.started":"2024-03-15T09:49:09.755470Z","shell.execute_reply":"2024-03-15T09:50:11.493181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport tempfile\nimport time\nimport matplotlib.pyplot as plt\n# from monai.apps import DecathlonDataset\nfrom monai.config import print_config\nfrom monai.data import DataLoader, decollate_batch\nfrom monai.handlers.utils import from_engine\nfrom monai.losses import DiceLoss\nfrom monai.inferers import sliding_window_inference\nfrom monai.metrics import DiceMetric\n# from monai.networks.nets import SegResNet\nfrom monai.transforms import (\n    Activations,\n    Activationsd,\n    AsDiscrete,\n    AsDiscreted,\n    Compose,\n    Invertd,\n    LoadImaged,\n    MapTransform,\n    NormalizeIntensityd,\n    Orientationd,\n    RandFlipd,\n    RandScaleIntensityd,\n    RandShiftIntensityd,\n    RandSpatialCropd,\n    Spacingd,\n    EnsureTyped,\n    EnsureChannelFirstd,\n)\nfrom monai.utils import set_determinism\n\nimport torch\n\nprint_config()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:11.496262Z","iopub.execute_input":"2024-03-15T09:50:11.497424Z","iopub.status.idle":"2024-03-15T09:50:11.540204Z","shell.execute_reply.started":"2024-03-15T09:50:11.497386Z","shell.execute_reply":"2024-03-15T09:50:11.539118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\nroot_dir = tempfile.mkdtemp() if directory is None else directory\nprint(root_dir)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:11.544425Z","iopub.execute_input":"2024-03-15T09:50:11.544760Z","iopub.status.idle":"2024-03-15T09:50:11.566544Z","shell.execute_reply.started":"2024-03-15T09:50:11.544733Z","shell.execute_reply":"2024-03-15T09:50:11.565284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_determinism(seed=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:11.568067Z","iopub.execute_input":"2024-03-15T09:50:11.568401Z","iopub.status.idle":"2024-03-15T09:50:11.581285Z","shell.execute_reply.started":"2024-03-15T09:50:11.568373Z","shell.execute_reply":"2024-03-15T09:50:11.580242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n    \"\"\"\n    Convert labels to multi channels based on brats classes:\n    label 1 is the peritumoral edema\n    label 2 is the GD-enhancing tumor\n    label 3 is the necrotic and non-enhancing tumor core\n    The possible classes are TC (Tumor core), WT (Whole tumor)\n    and ET (Enhancing tumor).\n\n    \"\"\"\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            result = []\n            # merge label 2 and label 3 to construct TC\n            result.append(torch.logical_or(d[key] == 2, d[key] == 3))\n            # merge labels 1, 2 and 3 to construct WT\n            result.append(torch.logical_or(torch.logical_or(d[key] == 2, d[key] == 3), d[key] == 1))\n            # label 2 is ET\n            result.append(d[key] == 2)\n            d[key] = torch.stack(result, axis=0).float()\n        return d","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:11.582719Z","iopub.execute_input":"2024-03-15T09:50:11.583130Z","iopub.status.idle":"2024-03-15T09:50:11.592748Z","shell.execute_reply.started":"2024-03-15T09:50:11.583077Z","shell.execute_reply":"2024-03-15T09:50:11.591783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transform = Compose(\n    [\n        # load 4 Nifti images and stack them together\n        LoadImaged(keys=[\"image\", \"label\"]),\n        EnsureChannelFirstd(keys=\"image\"),\n        EnsureTyped(keys=[\"image\", \"label\"]),\n        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n        Spacingd(\n            keys=[\"image\", \"label\"],\n            pixdim=(1.0, 1.0, 1.0),\n            mode=(\"bilinear\", \"nearest\"),\n        ),\n        RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=[128, 128, 128], random_size=False),\n        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n    ]\n)\nval_transform = Compose(\n    [\n        LoadImaged(keys=[\"image\", \"label\"]),\n        EnsureChannelFirstd(keys=\"image\"),\n        EnsureTyped(keys=[\"image\", \"label\"]),\n        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n        Spacingd(\n            keys=[\"image\", \"label\"],\n            pixdim=(1.0, 1.0, 1.0),\n            mode=(\"bilinear\", \"nearest\"),\n        ),\n        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:11.594095Z","iopub.execute_input":"2024-03-15T09:50:11.594446Z","iopub.status.idle":"2024-03-15T09:50:11.623603Z","shell.execute_reply.started":"2024-03-15T09:50:11.594418Z","shell.execute_reply":"2024-03-15T09:50:11.622083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# here we don't cache any data in case out of memory issue\ntrain_ds = DecathlonDataset(\n    root_dir=root_dir,\n    task=\"Task01_BrainTumour\",\n    transform=train_transform,\n    section=\"training\",\n    download=True,\n    cache_rate=0.0,\n    num_workers=4,\n)\ntrain_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)\nval_ds = DecathlonDataset(\n    root_dir=root_dir,\n    task=\"Task01_BrainTumour\",\n    transform=val_transform,\n    section=\"validation\",\n    download=False,\n    cache_rate=0.0,\n    num_workers=4,\n)\nval_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4)\n\ntest_ds = DecathlonDataset(\n    root_dir=root_dir,\n    task=\"Task01_BrainTumour\",\n    transform=val_transform,\n    section=\"test\",\n    download=False,\n    cache_rate=0.0,\n    num_workers=4,\n)\ntest_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=4)\n\nprint(len(train_ds), len(val_ds), len(test_ds))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:11.625244Z","iopub.execute_input":"2024-03-15T09:50:11.625700Z","iopub.status.idle":"2024-03-15T09:50:11.840430Z","shell.execute_reply.started":"2024-03-15T09:50:11.625659Z","shell.execute_reply":"2024-03-15T09:50:11.839177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pick one image from DecathlonDataset to visualize and check the 4 channels\nval_data_example = val_ds[2]\nprint(f\"image shape: {val_data_example['image'].shape}\")\nplt.figure(\"image\", (24, 6))\nfor i in range(4):\n    plt.subplot(1, 4, i + 1)\n    plt.title(f\"image channel {i}\")\n    plt.imshow(val_data_example[\"image\"][i, :, :, 60].detach().cpu(), cmap=\"gray\")\nplt.show()\n# also visualize the 3 channels label corresponding to this image\nprint(f\"label shape: {val_data_example['label'].shape}\")\nplt.figure(\"label\", (18, 6))\nfor i in range(3):\n    plt.subplot(1, 3, i + 1)\n    plt.title(f\"label channel {i}\")\n    plt.imshow(val_data_example[\"label\"][i, :, :, 60].detach().cpu())\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:11.842231Z","iopub.execute_input":"2024-03-15T09:50:11.844732Z","iopub.status.idle":"2024-03-15T09:50:16.367534Z","shell.execute_reply.started":"2024-03-15T09:50:11.844694Z","shell.execute_reply":"2024-03-15T09:50:16.366013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from monai.losses import DiceLoss\nfrom monai.metrics import DiceMetric\nfrom torchvision.transforms import Compose\nfrom monai.transforms import Compose, Activations, AsDiscrete\n\n\nmax_epochs = 35\nval_interval = 1\nVAL_AMP = True\ndevice=torch.device(\"cuda:0\")\nmodel = VTUNet(config=cfgs3).to(device)\ninputs = torch.rand(1, 4, 128, 128, 128).to(device)\nimage_size = 128\noutputs = model(inputs)\nprint(outputs.shape)\n# print(model)\nmodel.load_state_dict(torch.load(\"/kaggle/input/checkpoint/best_metric_model_140.pth\"))\n\nloss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\noptimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n\ndice_metric = DiceMetric(include_background=True, reduction=\"mean\")\ndice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n\npost_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n\n\n# define inference method\ndef inference(input):\n    def _compute(input):\n        return sliding_window_inference(\n            inputs=input,\n            roi_size=(128, 128, 128),\n            sw_batch_size=1,\n            predictor=model,\n            overlap=0.5,\n        )\n\n    if VAL_AMP:\n        with torch.cuda.amp.autocast():\n            return _compute(input)\n    else:\n        return _compute(input)\n\n\n# use amp to accelerate training\nscaler = torch.cuda.amp.GradScaler()\n# enable cuDNN benchmark\ntorch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:16.368922Z","iopub.execute_input":"2024-03-15T09:50:16.369298Z","iopub.status.idle":"2024-03-15T09:50:17.859295Z","shell.execute_reply.started":"2024-03-15T09:50:16.369268Z","shell.execute_reply":"2024-03-15T09:50:17.856984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_metric = -1\nbest_metric_epoch = -1\nbest_metrics_epochs_and_time = [[], [], []]\nepoch_loss_values = []\nval_epoch_loss_values = []\nmetric_values = []\ntrain_metric_values = []\nmetric_values_tc = []\nmetric_values_wt = []\nmetric_values_et = []\nimport time\n\ntotal_start = time.time()\n\nfor epoch in range(max_epochs):\n    epoch_start = time.time()\n    print(\"-\" * 10)\n    print(f\"epoch {epoch + 1}/{max_epochs}\")\n    model.train()\n    epoch_loss = 0\n    epoch_metric = 0\n    val_epoch_loss = 0\n    step = 0\n    for batch_data in train_loader:\n        step_start = time.time()\n        step += 1\n        inputs, labels = (\n            batch_data[\"image\"].to(device),\n            batch_data[\"label\"].to(device),\n        )\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast():\n            outputs = model(inputs)\n            loss = loss_function(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        epoch_loss += loss.item()\n        outputs = [post_trans(i) for i in decollate_batch(outputs)]\n        dice_metric(y_pred=outputs, y=labels)\n        train_metric = dice_metric.aggregate().item()\n        epoch_metric += train_metric\n        \n#         dice_metric_batch(y_pred=outputs, y=labels)\n        print(\n            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n            f\", train_loss: {loss.item():.4f}\"\n            f\", current mean dice: {train_metric:.4f}\"\n            f\", step time: {(time.time() - step_start):.4f}\"\n        )\n    lr_scheduler.step()\n    epoch_loss /= step\n    epoch_metric /= step\n    epoch_loss_values.append(epoch_loss)\n    train_metric_values.append(epoch_metric)\n    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\"\n          f\", average metric: {epoch_metric:.4f}\")\n\n    if (epoch + 1) % val_interval == 0:\n        model.eval()\n        with torch.no_grad():\n            val_step = 0\n            for val_data in val_loader:\n                val_step += 1\n                val_inputs, val_labels = (\n                    val_data[\"image\"].to(device),\n                    val_data[\"label\"].to(device),\n                )\n                val_outputs = inference(val_inputs)\n                loss = loss_function(val_outputs, val_labels)\n                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n                dice_metric(y_pred=val_outputs, y=val_labels)\n                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n                val_epoch_loss += loss.item()\n                \n            metric = dice_metric.aggregate().item()\n            metric_values.append(metric)\n            metric_batch = dice_metric_batch.aggregate()\n            metric_tc = metric_batch[0].item()\n            metric_values_tc.append(metric_tc)\n            metric_wt = metric_batch[1].item()\n            metric_values_wt.append(metric_wt)\n            metric_et = metric_batch[2].item()\n            metric_values_et.append(metric_et)\n            dice_metric.reset()\n            dice_metric_batch.reset()\n\n            if metric > best_metric:\n                best_metric = metric\n                best_metric_epoch = epoch + 1\n                best_metrics_epochs_and_time[0].append(best_metric)\n                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n                torch.save(\n                    model.state_dict(),\n                    os.path.join(\"/kaggle/working/\", \"best_metric_model.pth\"),\n                )\n                print(\"saved new best metric model\")\n            chk_file_name = \"epoch_\" + str(epoch) + \"_model.pth\"    \n            torch.save(\n                    model.state_dict(),\n                    os.path.join(\"/kaggle/working/\", chk_file_name),\n                )\n            val_epoch_loss /= val_step\n            val_epoch_loss_values.append(val_epoch_loss)\n            print(\n                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n                f\", val_loss: {val_epoch_loss:.4f}\"\n                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n                f\"\\nbest mean dice: {best_metric:.4f}\"\n                f\" at epoch: {best_metric_epoch}\"\n            )\n    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\ntotal_time = time.time() - total_start","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:17.860863Z","iopub.status.idle":"2024-03-15T09:50:17.861502Z","shell.execute_reply.started":"2024-03-15T09:50:17.861206Z","shell.execute_reply":"2024-03-15T09:50:17.861231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(val_epoch_loss_values), val_epoch_loss_values)\nprint(len(epoch_loss_values), epoch_loss_values)\nprint(len(train_metric_values), train_metric_values)\nprint(len(metric_values), metric_values)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:17.863050Z","iopub.status.idle":"2024-03-15T09:50:17.863651Z","shell.execute_reply.started":"2024-03-15T09:50:17.863355Z","shell.execute_reply":"2024-03-15T09:50:17.863380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\nsource_path = \"/kaggle/input/metrics-loss/SegResNet_data.csv\"\ndestination_path = \"/kaggle/working/SegResNet_data.csv\"\n\n# Copy the file\nshutil.copy(source_path, destination_path)\n\nprint(f\"File '{source_path}' copied to '{destination_path}'.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:17.865749Z","iopub.status.idle":"2024-03-15T09:50:17.866351Z","shell.execute_reply.started":"2024-03-15T09:50:17.866039Z","shell.execute_reply":"2024-03-15T09:50:17.866062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\n# Your lists of data\n# list1 = [1, 2, 3, 4, 5]\n# list2 = ['a', 'b', 'c', 'd', 'e']\n# list3 = [10, 20, 30, 40, 50]\n# list4 = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\n# CSV file path\ncsv_file_path = '/kaggle/working/SegResNet_data.csv'\n\n# Column names\nfieldnames = ['epoch_loss_values', 'train_metric_values', 'val_epoch_loss_values', 'metric_values']\n\n# Writing lists to a CSV file with specific column names\nwith open(csv_file_path, 'a', newline='') as csvfile:  # Change 'w' to 'a' for append mode\n    # Create a CSV writer object with DictWriter\n    csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n    # Write the lists to the specified columns\n    for i in range(len(epoch_loss_values)):\n        csv_writer.writerow({\n            'epoch_loss_values': epoch_loss_values[i],\n            'train_metric_values': train_metric_values[i],\n            'val_epoch_loss_values': val_epoch_loss_values[i],\n            'metric_values': metric_values[i]\n        })\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:17.868179Z","iopub.status.idle":"2024-03-15T09:50:17.868752Z","shell.execute_reply.started":"2024-03-15T09:50:17.868448Z","shell.execute_reply":"2024-03-15T09:50:17.868481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}, total time: {total_time}.\")","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:17.870490Z","iopub.status.idle":"2024-03-15T09:50:17.871064Z","shell.execute_reply.started":"2024-03-15T09:50:17.870773Z","shell.execute_reply":"2024-03-15T09:50:17.870797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(\"train\", (12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Epoch Average Loss\")\nx = [i + 1 for i in range(len(epoch_loss_values))]\ny = epoch_loss_values\nplt.xlabel(\"epoch\")\nplt.plot(x, y, color=\"red\")\nplt.subplot(1, 2, 2)\nplt.title(\"Val Mean Dice\")\nx = [val_interval * (i + 1) for i in range(len(metric_values))]\ny = metric_values\nplt.xlabel(\"epoch\")\nplt.plot(x, y, color=\"green\")\nplt.show()\n\nplt.figure(\"train\", (18, 6))\nplt.subplot(1, 3, 1)\nplt.title(\"Val Mean Dice TC\")\nx = [val_interval * (i + 1) for i in range(len(metric_values_tc))]\ny = metric_values_tc\nplt.xlabel(\"epoch\")\nplt.plot(x, y, color=\"blue\")\nplt.subplot(1, 3, 2)\nplt.title(\"Val Mean Dice WT\")\nx = [val_interval * (i + 1) for i in range(len(metric_values_wt))]\ny = metric_values_wt\nplt.xlabel(\"epoch\")\nplt.plot(x, y, color=\"brown\")\nplt.subplot(1, 3, 3)\nplt.title(\"Val Mean Dice ET\")\nx = [val_interval * (i + 1) for i in range(len(metric_values_et))]\ny = metric_values_et\nplt.xlabel(\"epoch\")\nplt.plot(x, y, color=\"purple\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:17.872600Z","iopub.status.idle":"2024-03-15T09:50:17.873185Z","shell.execute_reply.started":"2024-03-15T09:50:17.872871Z","shell.execute_reply":"2024-03-15T09:50:17.872893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install nibabel","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:17.875402Z","iopub.status.idle":"2024-03-15T09:50:17.875840Z","shell.execute_reply.started":"2024-03-15T09:50:17.875637Z","shell.execute_reply":"2024-03-15T09:50:17.875654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nibabel as nib \nimg_add = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-t1c.nii\"\nlabel_add = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-t1n.nii\"\nimg_add_2 = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-t2f.nii\"\nimg_add_3 = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-t2w.nii\"\nimg = nib.load(img_add).get_fdata()\nlabel = nib.load(label_add).get_fdata()\nimg_2 = nib.load(img_add_2).get_fdata()\nimg_3 = nib.load(img_add_3).get_fdata()\nprint(f\"image shape: {img.shape}, label shape: {label.shape}\")\nplt.figure(\"image\", (18, 6))\nplt.subplot(1, 4, 1)\nplt.title(\"BraTS-GLI-00814-000-t1c.nii\")\nplt.imshow(img[:, :, 78], cmap=\"gray\")\nplt.subplot(1, 4, 2)\nplt.title(\"BraTS-GLI-00814-000-t1n.nii\")\nplt.imshow(label[:, :, 78], cmap=\"gray\")\nplt.subplot(1, 4, 3)\nplt.title(\"BraTS-GLI-00814-000-t2f.nii\")\nplt.imshow(img_2[:, :, 78], cmap=\"gray\")\nplt.subplot(1, 4, 4)\nplt.title(\"BraTS-GLI-00814-000-t2w.nii\")\nplt.imshow(img_3[:, :, 78], cmap=\"gray\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:17.876956Z","iopub.status.idle":"2024-03-15T09:50:17.877391Z","shell.execute_reply.started":"2024-03-15T09:50:17.877192Z","shell.execute_reply":"2024-03-15T09:50:17.877209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nibabel as nib \nfrom matplotlib.colors import ListedColormap\nimg_add = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-t2f.nii\"\nlabel_add = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-seg.nii\"\nimg = nib.load(img_add).get_fdata()\nlabel = nib.load(label_add).get_fdata()\n# Create a custom colormap with purple, blue, and orange colors\ncmap = ListedColormap(['black', 'blue', 'orange', 'green'])\n\nprint(f\"image shape: {img.shape}, label shape: {label.shape}\")\nplt.figure(\"image\", (18, 6))\nplt.subplot(1, 3, 1)\nplt.title(\"BraTS-GLI-00814-000-t2f.nii\")\nplt.imshow(img[:, :, 78], cmap=\"gray\")\nplt.subplot(1, 3, 2)\nplt.title(\"BraTS-GLI-00814-000-seg.nii\")\nplt.imshow(label[:, :, 78])\nplt.subplot(1, 3, 3)\nplt.title(\"BraTS-GLI-00814-000-seg.nii\")\nplt.imshow(label[:, :, 78], cmap = cmap)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:17.878469Z","iopub.status.idle":"2024-03-15T09:50:17.878883Z","shell.execute_reply.started":"2024-03-15T09:50:17.878686Z","shell.execute_reply":"2024-03-15T09:50:17.878703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"/","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:17.880047Z","iopub.status.idle":"2024-03-15T09:50:17.880493Z","shell.execute_reply.started":"2024-03-15T09:50:17.880288Z","shell.execute_reply":"2024-03-15T09:50:17.880304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_org_transforms = Compose(\n    [\n        LoadImaged(keys=[\"image\", \"label\"]),\n        EnsureChannelFirstd(keys=[\"image\"]),\n        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n        Spacingd(keys=[\"image\"], pixdim=(1.0, 1.0, 1.0), mode=\"bilinear\"),\n        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n    ]\n)\n\npost_transforms = Compose(\n    [\n        Invertd(\n            keys=\"pred\",\n            transform=val_transform,\n            orig_keys=\"image\",\n            meta_keys=\"pred_meta_dict\",\n            orig_meta_keys=\"image_meta_dict\",\n            meta_key_postfix=\"meta_dict\",\n            nearest_interp=False,\n            to_tensor=True,\n            device=\"cpu\",\n        ),\n        Activationsd(keys=\"pred\", sigmoid=True),\n        AsDiscreted(keys=\"pred\", threshold=0.5),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:17.882861Z","iopub.status.idle":"2024-03-15T09:50:17.883308Z","shell.execute_reply.started":"2024-03-15T09:50:17.883077Z","shell.execute_reply":"2024-03-15T09:50:17.883094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from monai.metrics import HausdorffDistanceMetric\nfrom monai.metrics import SurfaceDiceMetric\nfrom monai.metrics import MeanIoU\nfrom monai.metrics import SurfaceDistanceMetric\nfrom monai.metrics import ROCAUCMetric\nfrom monai.metrics import SurfaceDiceMetric\n\nhd_metric = HausdorffDistanceMetric(include_background=False, reduction=\"mean\", percentile = 95)\nhd_metric_batch = HausdorffDistanceMetric(include_background=True, reduction=\"mean_batch\", percentile = 95)\n\nsd_metric = SurfaceDistanceMetric(include_background=True, reduction=\"mean\")\nsd_metric_batch = SurfaceDistanceMetric(include_background=True, reduction=\"mean_batch\")\n\nmeanIoU_metric = MeanIoU(include_background=True, reduction=\"mean\")\nmeanIoU_metric_batch = MeanIoU(include_background=True, reduction=\"mean_batch\")\n\nsurfaceDice_metric = SurfaceDiceMetric(include_background=True, reduction=\"mean\", class_thresholds = (0.01, 0.01, 0.01))\nsurfaceDice_metric_batch = SurfaceDiceMetric(include_background=True, reduction=\"mean_batch\", class_thresholds = (0.01, 0.01, 0.01))\n\nmodel.load_state_dict(torch.load(\"/kaggle/input/checkpoint/best_metric_model_140.pth\"))\nmodel.eval()\ncount = 0\nwith torch.no_grad():\n    for val_data in test_loader:\n        count = count + 1\n        print(count)\n#         val_inputs = val_data[\"image\"].to(device)\n#         val_data[\"pred\"] = inference(val_inputs)\n#         val_data = [post_trans(i) for i in decollate_batch(val_data)]\n#         val_outputs, val_labels = from_engine([\"pred\", \"label\"])(val_data)\n        val_inputs, val_labels = (\n            val_data[\"image\"].to(device),\n            val_data[\"label\"].to(device),\n        )\n        val_outputs = inference(val_inputs)\n        val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n        \n        dice_metric(y_pred=val_outputs, y=val_labels)\n        dice_metric_batch(y_pred=val_outputs, y=val_labels)\n        \n        hd_metric(y_pred=val_outputs, y=val_labels)\n        hd_metric_batch(y_pred=val_outputs, y=val_labels)\n        \n        meanIoU_metric(y_pred=val_outputs, y=val_labels)\n        meanIoU_metric_batch(y_pred=val_outputs, y=val_labels)\n        \n        sd_metric(y_pred=val_outputs, y=val_labels)\n        sd_metric_batch(y_pred=val_outputs, y=val_labels)\n        \n        surfaceDice_metric(y_pred=val_outputs, y=val_labels)\n        surfaceDice_metric_batch(y_pred=val_outputs, y=val_labels)\n        \n\n    metric_org = dice_metric.aggregate().item()\n    metric_batch_org = dice_metric_batch.aggregate()\n    \n    hd_metric_org = hd_metric.aggregate().item()\n    hd_metric_batch_org = hd_metric_batch.aggregate()\n    \n    meanIoU_metric_org = meanIoU_metric.aggregate().item()\n    meanIoU_metric_batch_org = meanIoU_metric_batch.aggregate()\n    \n    sd_metric_org = sd_metric.aggregate().item()\n    sd_metric_batch_org = sd_metric_batch.aggregate()\n    \n    surfaceDice_metric_org = surfaceDice_metric.aggregate().item()\n    surfaceDice_metric_batch_org = surfaceDice_metric_batch.aggregate()\n\n    dice_metric.reset()\n    dice_metric_batch.reset()\n    \n    hd_metric.reset()\n    hd_metric_batch.reset()\n    \n    meanIoU_metric.reset()\n    meanIoU_metric_batch.reset()\n    \n    sd_metric.reset()\n    sd_metric_batch.reset()\n    \n    surfaceDice_metric.reset()\n    surfaceDice_metric_batch.reset()\n\nmetric_tc, metric_wt, metric_et = metric_batch_org[0].item(), metric_batch_org[1].item(), metric_batch_org[2].item()\n\nhd_metric_tc, hd_metric_wt, hd_metric_et = hd_metric_batch_org[0].item(), hd_metric_batch_org[1].item(), hd_metric_batch_org[2].item()\n\nmeanIoU_metric_tc, meanIoU_metric_wt, meanIoU_metric_et = meanIoU_metric_batch_org[0].item(), meanIoU_metric_batch_org[1].item(), meanIoU_metric_batch_org[2].item()\n\nsd_metric_tc, sd_metric_wt, sd_metric_et = sd_metric_batch_org[0].item(), sd_metric_batch_org[1].item(), sd_metric_batch_org[2].item()\n\nsurfaceDice_metric_tc, surfaceDice_metric_wt, surfaceDice_metric_et = surfaceDice_metric_batch_org[0].item(), surfaceDice_metric_batch_org[1].item(), surfaceDice_metric_batch_org[2].item()\n\nprint(\"Metric on original image spacing: \", metric_org)\nprint(f\"metric_tc: {metric_tc:.4f}\")\nprint(f\"metric_wt: {metric_wt:.4f}\")\nprint(f\"metric_et: {metric_et:.4f}\")\n\nprint(\"HD Metric on original image spacing: \", hd_metric_org)\nprint(f\"HD metric_tc: {hd_metric_tc:.4f}\")\nprint(f\"HD metric_wt: {hd_metric_wt:.4f}\")\nprint(f\"HD metric_et: {hd_metric_et:.4f}\")\n\nprint(\"MeanIoU Metric on original image spacing: \", meanIoU_metric_org)\nprint(f\"MeanIoU metric_tc: {meanIoU_metric_tc:.4f}\")\nprint(f\"MeanIoU metric_wt: {meanIoU_metric_wt:.4f}\")\nprint(f\"MeanIoU metric_et: {meanIoU_metric_et:.4f}\")\n\nprint(\"SD Metric on original image spacing: \", sd_metric_org)\nprint(f\"SD metric_tc: {sd_metric_tc:.4f}\")\nprint(f\"SD metric_wt: {sd_metric_wt:.4f}\")\nprint(f\"SD metric_et: {sd_metric_et:.4f}\")\n\nprint(\"Surface Dice Metric on original image spacing: \", surfaceDice_metric_org)\nprint(f\"Surface Dice metric_tc: {surfaceDice_metric_tc:.4f}\")\nprint(f\"Surface Dice metric_wt: {surfaceDice_metric_wt:.4f}\")\nprint(f\"Surface Dice metric_et: {surfaceDice_metric_et:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:17.885343Z","iopub.status.idle":"2024-03-15T09:50:17.885821Z","shell.execute_reply.started":"2024-03-15T09:50:17.885616Z","shell.execute_reply":"2024-03-15T09:50:17.885633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if directory is None:\n    shutil.rmtree(root_dir)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:50:17.887528Z","iopub.status.idle":"2024-03-15T09:50:17.887994Z","shell.execute_reply.started":"2024-03-15T09:50:17.887784Z","shell.execute_reply":"2024-03-15T09:50:17.887801Z"},"trusted":true},"execution_count":null,"outputs":[]}]}